{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/yumengch/pyPDAF/blob/main/example/tutorial1_serial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PACl2a3eN4Sy"
   },
   "source": [
    "# Building an Assimilation System with pyPDAF without parallelisation\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBE84vfB5OY6"
   },
   "source": [
    "pyPDAF is a Python interface to PDAF (Parallel Data Assimilation Framework). The framework is mainly designed for ensemble data assimilation systems with high-dimensional complex weather and climae models. It has applications to both research and operational purposes. The Python interface allows for the use of PDAF in Python, a flexible and rich environment.\n",
    "\n",
    "As per its name, the framework is designed with the aim to implement efficient parallelised data assimilation system. In this practical, we provide a step-by-step tutorial on constructing a DA system with pyPDAF without using parallelisation. This tutorial is a simplification of the code used in `example/online` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-maLnxQirCAv"
   },
   "source": [
    "## Install pyPDAF\n",
    "---\n",
    "Before discussing the DA system, let us install pyPDAF first. If you are familiar with Python, you might have the package manager `conda` installed. This can be obtained using `anaconda` or `miniconda`. This functionality can be used on Google Colab as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6Es_i7wGBNI"
   },
   "source": [
    "### On local computer:\n",
    "\n",
    "In your terminal or anaconda prompt, run `conda create -n pypdaf -c conda-forge yumengch::pypdaf==1.0.0 conda-forge::jupyter`.\n",
    "\n",
    "You can then open this notebook using the command `jupyter notebook`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSsjNPl-F1On"
   },
   "source": [
    "### On Google Colab (skip this section when you're not using Google Colab):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpxjlTuLSZ8H"
   },
   "source": [
    "The following step will install `conda` on the Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcRt2oa8N3fo"
   },
   "outputs": [],
   "source": [
    "!pip install -q condacolab\n",
    "import condacolab\n",
    "condacolab.install()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXxhlQy_69bd"
   },
   "source": [
    "Now, we can install pyPDAF using conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-o9dcjXOaAJ"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# installation of pyPDAF\n",
    "!conda create -n pypdaf -c conda-forge yumengch::pypdaf==1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNy2ubUw7Bl3"
   },
   "source": [
    "To provide a better view of PDAF output, we have to use wurlitzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Observations\n",
    "\n",
    "In a typical data assimilation application, one usually has a model that can simulate the system of interest. For example, in the numerical weather prediction, an atmosphere model is available. The same applies to ocean, sea ice, biogeochemistry and land surface models. These models usually provide a priori estimate (forecast) of the state of the system in sequential data assimilation schemes, or an initial guess/background state in variational methods. The motivation of data assimilation system is to obtain a good estimate of the state of the system with observations, which makes up another important component of data assimilation.\n",
    "\n",
    "When constructing a data assimilation system in pyPDAF, providing the model state and observation as well as their connection and uncertainty information is the key task. Before we get into pyPDAF, let's look at the example model and observations in this tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cGn7y1usALz"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHTqZ_SkHyYL"
   },
   "source": [
    "In this tutorial, a 2D model that propagates a sine wave along  in a rectangular domain is used for demonstration purpose. The model integration can be represented by a `step` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ygfYHHWf_UgI"
   },
   "outputs": [],
   "source": [
    "\"\"\"define the model integration step\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "def step(field: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Roll array elements of i-th time step along a the first axis.\"\"\"\n",
    "    return np.roll(field, shift=1, axis=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the tutorial, the model has\n",
    "- Spatial Domain: Two dimensional domain grid domain with $(nx \\times ny) = (36 \\times 18)$ grid points\n",
    "- Total steps: we will run this model by `nsteps = 18` time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHBcpFqo4d-i"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# define the array for model field\n",
    "nsteps = 18 # total time steps\n",
    "nx = 36 # 36 columns\n",
    "ny = 18 # 18 rows\n",
    "# initial condition + 18 time steps, 18 rows and 36 columns\n",
    "field = np.zeros((nsteps + 1, ny, nx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of simplicity, a true initial condition is provided: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_TOr4rAIAsp",
    "outputId": "91c83738-a657-42aa-c5be-6be69ad9c4fb"
   },
   "outputs": [],
   "source": [
    "\"\"\" this is a way to get all required data without using git and introducing additional libraries\"\"\"\n",
    "import os\n",
    "import urllib.request\n",
    "# create input data directory\n",
    "os.makedirs('inputs_online', exist_ok=True)\n",
    "link_to_files = 'https://raw.githubusercontent.com/PDAF/PDAF/PDAF_V2.1/tutorial/inputs_online'\n",
    "# get the initial truth\n",
    "urllib.request.urlretrieve(f'{link_to_files}/true_initial.txt', os.path.join('inputs_online','true_initial.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KCHJ4YD-mEC"
   },
   "source": [
    "We can read the initial condition to model field array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XCiMLV48_U3o"
   },
   "outputs": [],
   "source": [
    "\"\"\"read the initial condition of the model field\"\"\"\n",
    "field[0] = np.loadtxt(os.path.join('inputs_online','true_initial.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5OhqIzCSwNij"
   },
   "source": [
    "Now, we can visualise the model evolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cz44hSkm19hg"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "fig = plt.figure('animation')\n",
    "ax = fig.add_subplot(111)\n",
    "pc = ax.pcolormesh(field[0], cmap='coolwarm', vmin=-1, vmax=1)\n",
    "fig.colorbar(pc, ax=ax)\n",
    "\n",
    "def draw_model(i):\n",
    "  \"\"\"Draw each model step\n",
    "  \"\"\"\n",
    "  # run the model\n",
    "  field[i+1] = step(field[i])\n",
    "  pc.set_array(field[i+1])\n",
    "  ax.set_title(f'Model step {i+1}')\n",
    "  return pc,\n",
    "\n",
    "# make an animation\n",
    "anim = animation.FuncAnimation(fig, draw_model, frames=nsteps, interval=1000, blit=True)\n",
    "plt.close(fig)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-mYku6H84ud"
   },
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations are given for each time steps stored in 'obs_step*.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"retrieve observations\"\"\"\n",
    "for i in range(nsteps):\n",
    "    urllib.request.urlretrieve(f'{link_to_files}/obs_step{i+1}.txt', os.path.join('inputs_online',f'obs_step{i+1}.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-mYku6H84ud"
   },
   "source": [
    "- Among 648 grid points only 28 grid points have observations. This is because most DA methods can estimate grid points without observations by assuming each grid points form a multi-variate Gaussian distribution.\n",
    "- Compared to the truth, these observations have an error of 0.5\n",
    "- The retrieved data use `-999` as missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bg1G9Jxo9KpD"
   },
   "outputs": [],
   "source": [
    "# define observation array\n",
    "obs = np.ma.zeros((nsteps, ny, nx))\n",
    "fig = plt.figure('animationObs')\n",
    "ax = fig.add_subplot(111)\n",
    "pc = ax.pcolormesh(obs[0], cmap='coolwarm', vmin=-1, vmax=1)\n",
    "fig.colorbar(pc, ax=ax)\n",
    "def draw_model(i):\n",
    "  \"\"\"Draw obs. at each model step\n",
    "  \"\"\"\n",
    "  obs[i] = np.loadtxt(os.path.join('inputs_online', f'obs_step{i+1}.txt'))\n",
    "  obs[i] = np.ma.masked_where(np.isclose(obs[i], -999.), obs[i])\n",
    "  pc.set_array(obs[i])\n",
    "  ax.set_title(f'Observation at step {i+1}')\n",
    "  return pc,\n",
    "\n",
    "# make an animation\n",
    "anim = animation.FuncAnimation(fig, draw_model, frames=18, interval=1000, blit=True)\n",
    "plt.close(fig)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8yt_y_LNnVt"
   },
   "source": [
    "In this tutorial, we can actually obtain the truth,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(18):\n",
    "    # get all truth\n",
    "    urllib.request.urlretrieve(f'{link_to_files}/true_step{i+1}.txt', os.path.join('inputs_online', f'true_step{i+1}.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison to the truth, we can check the observation error and calculate it as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5U5Go0lKMRbH"
   },
   "outputs": [],
   "source": [
    "# calculate the root mean squared obs. err in time\n",
    "err = np.sqrt(np.sum((field[1:] - obs)**2, axis=0)/nsteps)\n",
    "# plot the observation error\n",
    "fig = plt.figure('R')\n",
    "ax = fig.add_subplot(111)\n",
    "pc = ax.pcolormesh(err, cmap='Blues', vmin=0., vmax=1.)\n",
    "ax.set_title(f'Spatial averaged obs. err is {np.round(np.mean(err), 3)}')\n",
    "fig.colorbar(pc, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfRF0cTmPWI4"
   },
   "source": [
    "## Set up a data assimilation system using pyPDAF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can construct our data assimilation systems. As in many other Python programs, we use third-party packages for the system. This is one of the benefits of a Python system as many functionalities are available. In this tutorial, `pyPDAF`, `numpy` and `mpi4py` is used. Note that even though parallel features are not used in this tutorial, `mpi4py` is still required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\"\"\"For the sake of compatibility of running it in Google Colab, we install wurlitzer here. \n",
    "   This is not essential if you run it on your local computer, but you will need to remove\n",
    "   all code related to wurlitzer in this notebook.\"\"\"\n",
    "# wurlitzer is a package that allows us to see PDAF output\n",
    "!pip install wurlitzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pzlLMXevPF8I"
   },
   "outputs": [],
   "source": [
    "import pyPDAF.PDAF as PDAF\n",
    "import mpi4py.MPI as MPI\n",
    "import numpy as np\n",
    "from wurlitzer import pipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gwe4J1xVfGTK"
   },
   "source": [
    "### Initialise PDAF\n",
    "\n",
    "The initialisation of PDAF tells PDAF the our choice of data assimilation algorithms, ensemble size, inflation factor, and the dimension of the state vector. Currently (up to PDAF V2.3.1), any DA methods in PDAF require these options. However, some methods can only be executed with additional parameters. This is done by [`PDAF.init`](https://yumengch.github.io/pyPDAF/PDAF.html#pyPDAF.PDAF.init).\n",
    "\n",
    "In this tutorial, the **error space transform Kalman filter (ESTKF)** is used with **9** ensemble members. We will estimate the state of every model grid point, which gives us a state vector with the size of nx × ny = 36 × 18 = 648. The `filtertype` and `subtype` here specifies the DA method and will be given as an argument in `pyPDAF.PDAF.init` function. A full list of supported methods can be found in [PDAF wiki](https://pdaf.awi.de/trac/wiki/AvailableOptionsforInitPDAF). We first put these information into Python variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g4wDabInPY7O"
   },
   "outputs": [],
   "source": [
    "# using error space transform Kalman filter (ESTKF)\n",
    "filtertype = 6\n",
    "# standard form\n",
    "subtype = 0\n",
    "# dimension of the state vector\n",
    "# if model is parallelised, this is the dimension of state vector on each process\n",
    "dim_state_p = nx*ny\n",
    "# number of ensemble members\n",
    "dim_ens = 9\n",
    "# forget factor\n",
    "forget_factor = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrIrGKSvWrcu"
   },
   "source": [
    "In addition to the above information, [`PDAF.init`](https://yumengch.github.io/pyPDAF/PDAF.html#pyPDAF.PDAF.init) also asks for an **initial ensemble**. This information is given by the user-supplied function. These functions have fixed interface. Therefore\n",
    "- the input arguments and return variables should not be changed.\n",
    "- only the value of returned variables should be changed\n",
    "\n",
    "Documentation of the input arguments and return variable of this function can be found in [pyPDAF documentation](https://yumengch.github.io/pyPDAF/_autosummary/pyPDAF.PDAF.init.html#pyPDAF.PDAF.init). In this user-supplied function, the primary purpose is to fill the `ens_p` array. The `ens_p` is an array allocated by PDAF to perform data assimilation. Pure input arguments such as `filtertype`, `dim_p`, and `dim_ens` are given by PDAF. These information is given to PDAF when we call `PDAF.init`. When we implement the user-supplied function, they can be difficult to access. Therefore, PDAF provide these variables as input argument in the user-supplied function. For example, in this tutorial, the initial ensemble is read from text files. The `dim_ens` argument helps us the set up a loop to read these text files.\n",
    "\n",
    "In real applications, we may encouter different scenarios. For example, one may need to use algorithms to generate perturbations to create an initial ensemble from a model trajectory, see [PDAF functionality](https://pdaf.awi.de/trac/wiki/EnsembleGeneration). If one have an ensemble of model restart file, we can also simply return the variables in this user-supplied function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LIksYDywTetx"
   },
   "outputs": [],
   "source": [
    "def init_ens_pdaf(filtertype, dim_p, dim_ens, state_p, uinv, ens_p, status_pdaf):\n",
    "    \"\"\"Here, only ens_p variable matters while dim_p and dim_ens defines the\n",
    "    size of the variables. uinv, state_p are not used in this example.\n",
    "\n",
    "    status_pdaf is used to handle errors which we will not do it in this example.\n",
    "    \"\"\"\n",
    "    # get initial ensemble\n",
    "    for i in range(dim_ens):\n",
    "        urllib.request.urlretrieve(f'{link_to_files}/ens_{i+1}.txt', os.path.join('inputs_online', f'ens_{i+1}.txt'))    \n",
    "        ens_p[:, i] = np.loadtxt(os.path.join('inputs_online', f'ens_{i+1}.txt')).ravel()\n",
    "    return state_p, uinv, ens_p, status_pdaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tf8x0NGAfEz2"
   },
   "source": [
    "With these information, we can call PDAF function [`PDAF.init`](https://yumengch.github.io/pyPDAF/PDAF.html#pyPDAF.PDAF.init) to initialise the DA system. In this serial example, we specify all MPI communicators as `MPI.COMM_WORD` with one parallel model task with `task_id=1`. The single process is also used to perform filtering so that `in_filterpe=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tb76E2-OPOPU"
   },
   "outputs": [],
   "source": [
    "# this gives the verbose level of the PDAF, here we use 3 which is very verbose\n",
    "screen = 3\n",
    "# current step of the model which is 0\n",
    "current_step = 0\n",
    "# initialise the MPI package\n",
    "if not MPI.Is_initialized():\n",
    "    MPI.Init()\n",
    "with pipes() as (out, err):\n",
    "    _, _, status = PDAF.init(filtertype, subtype, current_step,\n",
    "                            np.array([dim_state_p, dim_ens], dtype=np.intc),\n",
    "                            np.array([forget_factor, ]),\n",
    "                            MPI.COMM_WORLD.py2f(), MPI.COMM_WORLD.py2f(),\n",
    "                            MPI.COMM_WORLD.py2f(),\n",
    "                            task_id=1, n_modeltasks=1, in_filterpe=True,\n",
    "                            py__init_ens_pdaf=init_ens_pdaf,\n",
    "                            in_screen=screen)\n",
    "# print PDAF screen output\n",
    "print (out.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXpuKapDbnZN"
   },
   "source": [
    "### Distribution of the ensemble from PDAF\n",
    "After PDAF initialisation, PDAF should distribute the ensemble back to the model to initialise the following forecast. This is accomplished by [`pyPDAF.PDAF.get_state`](https://yumengch.github.io/pyPDAF/_autosummary/pyPDAF.PDAF.get_state.html#pyPDAF.PDAF.get_state) function. \n",
    "\n",
    "This function depends on three user-supplied functions that are executed in the following sequence:\n",
    "1. Processing the initial PDAF ensemble by user-supplied function `py__prepoststep_state_pdaf`. This is because the ensemble stored in PDAF may not satisify some physical constraints such as balance conditions (hydrostatic balance/constitutive relations) or boundness (percentage values/chemical concentrations).\n",
    "2. PDAF should distribute the initial ensemble to the model for following model forecasts in `py__distribute_state_pdaf`. This function relates the model field to the state vector used by PDAF.\n",
    "3. PDAF perform actual data assimilation based on its internal counter for the time steps. In `pyPDAF.PDAF.init` function, we specify the initial time step. Now, a user-supplied function informs PDAF the data assimilation is performed after `nsteps` of forecast in `py__next_observation_pdaf`. The internal time step counter is incremented by one whenever one calls the assimilation functions for [fully parallel DA algorithms](https://yumengch.github.io/pyPDAF/API.html#fully-parallel-da-algorithms). This means that data assimilation will be performed when one of the assimilation functions is called at the `nsteps`-th step. For example, if we set `nsteps=2`, [`pyPDAF.PDAF.omi_assimilate_global`](https://yumengch.github.io/pyPDAF/_autosummary/pyPDAF.PDAF.omi_assimilate_global.html#pyPDAF.PDAF.omi_assimilate_global) only execute DA when it is called the second time in a loop. This, however, does not apply for [flexible DA algorithms](https://yumengch.github.io/pyPDAF/API.html#flexible-da-algorithms) which is used in this tutorial.\n",
    "\n",
    "To implement these user-supplied functions, we define a `PdafDistributor` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5L5eR_B6ZVq"
   },
   "outputs": [],
   "source": [
    "class PdafDistributor:\n",
    "    \"\"\"Distribute the ensemble members to the model tasks\n",
    "    \"\"\"\n",
    "    def __init__(self, nx, ny, dim_ens):\n",
    "        # counter for the i-th ensemble member when distribute\n",
    "        self.i_ens_pdaf = 0\n",
    "        # define the model field based on the ensemble\n",
    "        self.nx, self.ny = nx, ny\n",
    "        self.field = np.zeros((dim_ens, ny, nx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_44HquA6aXw"
   },
   "source": [
    "In this non-parallel code, PDAF will distribute ensemble members one by one from the input argument `state_p` with `distribute_state` method. We can keep track of the current ensemble member by a counter `self.i_ens_pdaf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cQCHlYMxc-Rp"
   },
   "outputs": [],
   "source": [
    "class PdafDistributor(PdafDistributor):\n",
    "    def distribute_state(self, dim_p, state_p):\n",
    "        \"\"\"PDAF will distribute state vector (state_p) to model field\n",
    "        \"\"\"\n",
    "        self.field[self.i_ens_pdaf] = state_p[:].reshape((self.ny, self.nx))\n",
    "        self.i_ens_pdaf += 1\n",
    "        return state_p\n",
    "\n",
    "    def reset_ens_index(self):\n",
    "        \"\"\"reset ensemble index to 0\n",
    "        \"\"\"\n",
    "        self.i_ens_pdaf = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzC5mODK6fxJ"
   },
   "source": [
    "In this simple model example, there are no need for actual processing of the ensemble. Therefore, we only show screen output of root mean squared error based on sampled variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1lgaGqK6k7h"
   },
   "outputs": [],
   "source": [
    "class PdafDistributor(PdafDistributor):\n",
    "    def initial_process(self, step, dim_p, dim_ens, dim_ens_l, dim_obs_p, state_p, uinv, ens_p, flag):\n",
    "        \"\"\"initial processing of the ensemble before it is distributed to model fields\n",
    "        \"\"\"\n",
    "        print (f'RMS error according to sampled variance: {np.sqrt(np.mean(np.var(ens_p, axis=1)))}')\n",
    "        return state_p, uinv, ens_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4h3TACpstgl"
   },
   "source": [
    "When obtaining the initial ensemble, users also need to provide information about when do we do the next analysis based on the arrival of the new observations. In our case, we have observations for each time step, but we'd like to assimilate it every other time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DbmL_zvjYJaa"
   },
   "outputs": [],
   "source": [
    "class PdafDistributor(PdafDistributor):\n",
    "    def next_observation(self, stepnow, nsteps, doexit, time):\n",
    "        # next observation will arrive at `nsteps' step\n",
    "        nsteps = 2\n",
    "        # doexit = 0 means that PDAF will continue to distribute state\n",
    "        # to model for further integrations\n",
    "        doexit = 0\n",
    "        # model time is not used here as we only use steps to define the time\n",
    "        return nsteps, doexit, time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bu157WPn7CiJ"
   },
   "source": [
    "Here, we call pyPDAF's [`get_state`](https://yumengch.github.io/pyPDAF/PDAF.html#pyPDAF.PDAF.get_state) function where it will also execute our user-supplied functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HX5TjUCNYTVh"
   },
   "outputs": [],
   "source": [
    "status = 0\n",
    "distributor = PdafDistributor(nx, ny, dim_ens)\n",
    "# making sure that distributor.i_ens_pdaf = 0\n",
    "# before state vector distribution\n",
    "distributor.reset_ens_index()\n",
    "steps_for = 0\n",
    "doexit = 0\n",
    "# loop over all dimensions\n",
    "for i in range(dim_ens):\n",
    "    with pipes() as (out, err):\n",
    "        steps_for, time, doexit, status = PDAF.get_state(steps_for, doexit,\n",
    "                                          distributor.next_observation,\n",
    "                                          distributor.distribute_state,\n",
    "                                          distributor.initial_process,\n",
    "                                          status)\n",
    "    print (out.read())\n",
    "# put model variable in distributor back to model\n",
    "field = distributor.field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLY8FDiL4NdC"
   },
   "source": [
    "The `get_state` function returns `steps_for`, `time` and `doexit` variable, which are given in `next_observation` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGeTvOG2u8LU"
   },
   "source": [
    "### Sequential data assimilation system\n",
    "\n",
    "Data assimilation combines the model forecast and the observations. Hence, to perform data assimilation, at each analysis step, PDAF must collect the new forecast from the model and read observations. The assimilation also needs a function that connects the forecast state to observations which usually is denoted as observation operator. To ensure the flexibility of the framework, these information depends on the user-supplied functions. This is because each observation type and model are different so one should expect the users can handle these information.\n",
    "\n",
    "In the serial system, the cycling can be divided into following steps:\n",
    "1. model forecast using `step` function\n",
    "2. put model forecast into PDAF state vector/ensemble array to perform DA using [`pyPDAF.PDAF.omi_put_state_global`](https://yumengch.github.io/pyPDAF/_autosummary/pyPDAF.PDAF.omi_put_state_global.html#pyPDAF.PDAF.omi_put_state_global).\n",
    "3. get the analysis state vector to initialise the next model forecast using [`pyPDAF.PDAF.get_state`](https://yumengch.github.io/pyPDAF/_autosummary/pyPDAF.PDAF.get_state.html#pyPDAF.PDAF.get_state).\n",
    "\n",
    "In above steps, the model forecast function is implemented in [`step`](#Model) function. Here, we demonstrate the user-supplied functions used by `pyPDAF.PDAF.omi_put_state_global`.\n",
    "\n",
    "#### User-supplied functions for `pyPDAF.PDAF.omi_put_state_global`\n",
    "In the assimilation step, the two primary purposes is defined by `PdafCollector` and `Obs` classes. The `PdafCollector` class obtains the model forecast and the `Obs` will handle observations using the `Observation Module Infrastructure` in PDAF, a scheme to ease the difficulty in handling observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-lmJYuKLE4z"
   },
   "outputs": [],
   "source": [
    "class PdafCollector:\n",
    "    def __init__(self, nx, ny, field):\n",
    "        # counter for the i-th ensemble member when distribute\n",
    "        self.i_ens_pdaf = 0\n",
    "        self.nx = nx\n",
    "        self.ny = ny\n",
    "        # define the model field based on the ensemble\n",
    "        self.field = field\n",
    "\n",
    "class Obs:\n",
    "    def __init__(self, i_obs):\n",
    "        # i_obs-th observations in the system starting from 1\n",
    "        self.i_obs = i_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PcPa7_IwLuFT"
   },
   "source": [
    "##### Collecting forecast\n",
    "Before going into the details of observation handling, we first\n",
    "1. get functions that collects the model forecast [(`collect_state_pdaf`)](https://yumengch.github.io/pyPDAF/UserFunc.html#pyPDAF.UserFunc.py__collect_state). Similar to `distribute_state_pdaf`, the state vector is collected for each ensemble member from model field.\n",
    "2. preprocess the ensemble forecast before the data assimilation. In the pre-processing step, we calculate the forecast error and save the forecast ensemble. This is the last step before assimilation so it could help us understand the raw forecast data. In the pre-process, the `step` argument is negative\n",
    "3. post-process the analysis ensemble after the assimilation. In this case, the `step` argument is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbv-1b7GsxU1"
   },
   "outputs": [],
   "source": [
    "class PdafCollector(PdafCollector):\n",
    "    def collect_state(self, dim_p, state_p):\n",
    "        \"\"\"PDAF will collect state vector (state_p) from model field\n",
    "        \"\"\"\n",
    "        state_p[:] = self.field[self.i_ens_pdaf].ravel()\n",
    "        self.i_ens_pdaf += 1\n",
    "        return state_p\n",
    "\n",
    "    def reset_ens_index(self):\n",
    "        \"\"\"reset ensemble index to 0\n",
    "        \"\"\"\n",
    "        self.i_ens_pdaf = 0\n",
    "\n",
    "    def preprocess(self, step, dim_ens, ens_p):\n",
    "        \"\"\"preprocessing of the ensemble before it is used by DA algorithms\n",
    "        \"\"\"\n",
    "        print (f'Forecast RMS error according to sampled variance: {np.sqrt(np.mean(np.var(ens_p, axis=1)))}')\n",
    "        for i in range(dim_ens):\n",
    "            np.savetxt(os.path.join('outputs', f'ens_{i+1}_step{-step}_for.txt') , ens_p[:, i].reshape((self.ny, self.nx)) )\n",
    "\n",
    "    def postprocess(self, step, dim_ens, ens_p):\n",
    "        \"\"\"post-processing of the ensemble before it is distributed to model fields\n",
    "        \"\"\"\n",
    "        print (f'Analysis RMS error according to sampled variance: {np.sqrt(np.mean(np.var(ens_p, axis=1)))}')\n",
    "        for i in range(dim_ens):\n",
    "            np.savetxt(os.path.join('outputs', f'ens_{i+1}_step{step}_ana.txt' ), ens_p[:, i].reshape((self.ny, self.nx)) )\n",
    "\n",
    "    def prepostprocess(self, step, dim_p, dim_ens, dim_ens_p, dim_obs_p, state_p, uinv, ens_p, flag):\n",
    "        if step < 0:\n",
    "          self.preprocess(step, dim_ens, ens_p)\n",
    "        else:\n",
    "          self.postprocess(step, dim_ens, ens_p)\n",
    "        return state_p, uinv, ens_p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1N-wjlkH3fSp"
   },
   "source": [
    "##### Handling observations\n",
    "Another essential ingredient of data assimilation is observation. Here, user-supplied functions give all information about the observations to PDAF. We use the OMI scheme in PDAF to handle observations. Without any localisations, only two user-supplied functions are required with the OMI scheme.\n",
    "\n",
    "Before we use the OMI scheme, we need to provide the number of observation types by [`PDAF.omi_init`](https://yumengch.github.io/pyPDAF/PDAF.html#pyPDAF.PDAF.omi_init). This function \n",
    "\n",
    "Here, we use only one type of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YiHIwd7tDHW"
   },
   "outputs": [],
   "source": [
    "PDAF.omi_init(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIExW3FhspP9"
   },
   "source": [
    "In this very simple example, the OMI observation functions may look a bit verbose, but it can be useful for more complex systems.\n",
    "\n",
    "The OMI has four mandatory properties:\n",
    "- [`doassim`](https://yumengch.github.io/pyPDAF/PDAF.html#pyPDAF.PDAF.omi_set_doassim): whether this observation is assimilated. If `doassim = 1`, this observation will be assimilated. If `doassim = 0`, it will not be assimilated.\n",
    "- [`disttype`](https://yumengch.github.io/pyPDAF/PDAF.html#pyPDAF.PDAF.omi_set_disttype): In localisation, how do we calculate the distance between grid points? e.g., Cartesian, geographic, or great circle distance on a sphere. We do not use localisation in this example, but we still have to provide this option.\n",
    "- [`ncoord`](https://yumengch.github.io/pyPDAF/PDAF.html#pyPDAF.PDAF.omi_set_ncoord): Number of coordinates used for computation in localisation. In our example, as we have a 2D domain, the number should be 2.\n",
    "- [`id_obs_p`](https://yumengch.github.io/pyPDAF/PDAF.html#pyPDAF.PDAF.omi_set_id_obs_p): Indices of observed field in state vector. This is a 2D array that should have the same length as the observector vector for each dimension. If the observations do not need interpolation (e.g., observations are co-located with model grid points), the first dimension is 1. In this case, if the i-th observation is at the j-th element of the state vector, the i-th element of `id_obs_p` is `j`. If interpolation is needed, each dimension is the adjacent model grid points.\n",
    "\n",
    "In Fortran, these properties can be given to derived type `obs_f`. In the pyPDAF, setter functions are provided. In the [`PDAF.omi_gather_obs`](https://yumengch.github.io/pyPDAF/PDAF.html#pyPDAF.PDAF.omi_gather_obs) function, PDAFomi collects \n",
    "- the observation vector\n",
    "- error variance\n",
    "- the spatial coordinate of the observations\n",
    "\n",
    "This function also returns the dimension of the observation for given observation type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-7PUdExu6FO"
   },
   "outputs": [],
   "source": [
    "class Obs(Obs):\n",
    "    def init_dim(self, step, dim_obs):\n",
    "        # We always assimilate the observation\n",
    "        PDAF.omi_set_doassim(self.i_obs, 1)\n",
    "        # Type of distance computation to use for localization\n",
    "        # It is mandatory for OMI even if we don't use localisation\n",
    "        PDAF.omi_set_disttype(self.i_obs, 0)\n",
    "        # Number of coordinates use for distance computation\n",
    "        PDAF.omi_set_ncoord(self.i_obs, 2)\n",
    "\n",
    "        # read observations\n",
    "        obs = np.loadtxt(os.path.join('inputs_online', f'obs_step{step}.txt'))\n",
    "        # get the dimension of the model grid\n",
    "        ny, nx = obs.shape\n",
    "        # flatten the observations\n",
    "        obs = obs.ravel()\n",
    "        # a mask for observed gridpoints\n",
    "        condition = np.logical_not(np.isclose(obs, -999))\n",
    "\n",
    "        # observation vector\n",
    "        y = obs[condition]\n",
    "\n",
    "        # The relationship between observation and state vector\n",
    "        # we only have 28 osbervations and each observation corresponds to\n",
    "        # the grid point of one element in the state vector\n",
    "        # id_obs_p gives the indices of observed field in state vector\n",
    "        # the id starts from 1\n",
    "        id_obs_p = np.zeros((1, len(y)), dtype=np.intc)\n",
    "        id_obs_p[0] = np.arange(1, len(obs) + 1, dtype=np.intc)[condition]\n",
    "        PDAF.omi_set_id_obs_p(self.i_obs, id_obs_p)\n",
    "\n",
    "        # inverse of observation variance\n",
    "        ivar_obs_p = 1./0.5/0.5*np.ones_like(y)\n",
    "\n",
    "        # coordinate of each observations\n",
    "        ocoord_p = np.zeros((2, len(y)))\n",
    "        ocoord_p[0] = np.tile(np.arange(nx), ny)[condition]\n",
    "        ocoord_p[1] = np.repeat(np.arange(ny), nx)[condition]\n",
    "\n",
    "        # not being used here, only used for localisation\n",
    "        local_range = 0.\n",
    "        dim_obs = PDAF.omi_gather_obs(self.i_obs, y,\n",
    "                                     ivar_obs_p, ocoord_p, local_range)\n",
    "        return dim_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXejHv4bl9qp"
   },
   "source": [
    "The other user-supplied function in this example will be the observation operator. In this simple example, the state vector in the observation space can be conveniently obtained by the OMI function [`pyPDAF.PDAF.omi_obs_op_gridpoint`](https://yumengch.github.io/pyPDAF/_autosummary/pyPDAF.PDAF.omi_obs_op_gridpoint.html#pyPDAF.PDAF.omi_obs_op_gridpoint) using the information provided in the `init_dim_obs` function. This function provides an observation operator where observations are located on the model grid points. When observations are not located on model grid points, PDAFomi also provides functions using linear interpolations on various grid. In this case, one may need to refer to [PDAF documentation](https://pdaf.awi.de/trac/wiki/OMI_observation_operators#Initializinginterpolationcoefficients) and [`id_obs_p` doc](https://yumengch.github.io/pyPDAF/_autosummary/pyPDAF.PDAF.omi_set_id_obs_p.html#pyPDAF.PDAF.omi_set_id_obs_p) for a better explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DBfL7xBQl-Nt"
   },
   "outputs": [],
   "source": [
    "class Obs(Obs):\n",
    "    def op(self, step, dim_p, dim_obs_p, state_p, ostate):\n",
    "        \"\"\"observation operator\n",
    "        \"\"\"\n",
    "        return PDAF.omi_obs_op_gridpoint(self.i_obs, state_p, ostate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User-supplied functions for `pyPDAF.PDAF.get_state`\n",
    "\n",
    "User-supplied functions for [`pyPDAF.PDAF.get_state`](https://yumengch.github.io/pyPDAF/_autosummary/pyPDAF.PDAF.get_state.html#pyPDAF.PDAF.get_state) will be the same for [previous section](#Distribution-of-the-ensemble-from-PDAF). However, in this case, the `initial_process` function will not be executed. Therefore, we make no changes in the `PdafDistributor` class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3V8ogKsh8Nq"
   },
   "source": [
    "##### Forward loop\n",
    "\n",
    "Now, we can write code for the sequential DA system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51H_l8TuwSBk"
   },
   "outputs": [],
   "source": [
    "os.makedirs('outputs', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PIz-l6zaZMNo"
   },
   "outputs": [],
   "source": [
    "# create directory to\n",
    "current_step = 0\n",
    "# full DA system integration loop\n",
    "while current_step < nsteps:\n",
    "\n",
    "    # model integration\n",
    "    for _ in range(steps_for):\n",
    "        field = step(field)\n",
    "        current_step += 1\n",
    "\n",
    "    # PDAF does assimilation\n",
    "    collector = PdafCollector(nx, ny, field)\n",
    "    obs = Obs(1)\n",
    "    collector.reset_ens_index()\n",
    "    for i in range(dim_ens):\n",
    "        with pipes() as (out, err):\n",
    "            status = PDAF.omi_put_state_global(collector.collect_state,\n",
    "                                      obs.init_dim, obs.op,\n",
    "                                      collector.prepostprocess)\n",
    "        print (out.read())\n",
    "\n",
    "    # PDAF distribute analysis back to model\n",
    "    distributor = PdafDistributor(nx, ny, dim_ens)\n",
    "    distributor.reset_ens_index()\n",
    "    for i in range(dim_ens):\n",
    "        with pipes() as (out, err):\n",
    "            # here, the distributor does not call initial_process function at all\n",
    "            # as it is only called when it is called by the first time.\n",
    "            steps_for, time, doexit, status = PDAF.get_state(steps_for, doexit,\n",
    "                                              distributor.next_observation,\n",
    "                                              distributor.distribute_state,\n",
    "                                              distributor.initial_process,\n",
    "                                              status)\n",
    "        print (out.read())\n",
    "    field = distributor.field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalise PDAF\n",
    "\n",
    "At the end of the data assimilation program, one should finalise the PDAF system using [`pyPDAF.PDAF.deallocate`](https://yumengch.github.io/pyPDAF/_autosummary/pyPDAF.PDAF.deallocate.html#pyPDAF.PDAF.deallocate). One can also obtain screen output of the computational time and memory use information using [pyPDAF.PDAF.print_info](https://yumengch.github.io/pyPDAF/_autosummary/pyPDAF.PDAF.print_info.html#pyPDAF.PDAF.print_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pipes() as (out, err):\n",
    "    PDAF.deallocate()\n",
    "    PDAF.print_info(1)\n",
    "    print (out.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTBiN9NAR1M1"
   },
   "source": [
    "### Does analysis look better than forecast?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0qoAUD5-R0BH"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "dim_ens=9\n",
    "ny, nx = 18, 36\n",
    "# define diagnotics and model fields\n",
    "spread = {'fcst': np.zeros(9), 'ana': np.zeros(9)}\n",
    "RMSE = {'fcst': np.zeros(9), 'ana': np.zeros(9)}\n",
    "field = {'truth': np.zeros((ny, nx)),\n",
    "         'fcst': np.zeros((dim_ens, ny, nx)),\n",
    "         'ana': np.zeros((dim_ens, ny, nx))\n",
    "         }\n",
    "for key in spread:\n",
    "    spread[key][:] = np.nan\n",
    "    RMSE[key][:] = np.nan\n",
    "# time\n",
    "time = np.arange(2, 20, 2)\n",
    "\n",
    "# get figure\n",
    "fig = plt.figure('err')\n",
    "w, h = fig.get_size_inches()\n",
    "fig.set_size_inches(2*w, 2*h)\n",
    "# define the time series plot\n",
    "ax = fig.add_subplot(212)\n",
    "ax.set_title('Time series of the ensemble spread and RMSE')\n",
    "ax.set_ylim([0., 1.2])\n",
    "ax.set_xlim([time[0] - 1, time[-1] + 1])\n",
    "lines = []\n",
    "for key, c in zip(spread, ['k', 'r']):\n",
    "    line, = ax.plot(time, spread[key], color=c, linestyle='dashed',label=f'{key} spread')\n",
    "    lines.append(line)\n",
    "    line, = ax.plot(time, RMSE[key], color=c, linestyle='solid',label=f'{key} RMSE')\n",
    "    lines.append(line)\n",
    "ax.legend()\n",
    "# define pcolormesh plots\n",
    "ax = {'fcst': fig.add_subplot(221), 'ana': fig.add_subplot(222),}\n",
    "pc = dict()\n",
    "for key in ax:\n",
    "    pc[key] = ax[key].pcolormesh(field[key].mean(0) - field['truth'],\n",
    "                                 cmap='coolwarm', vmin=-.06, vmax=.06)\n",
    "    fig.colorbar(pc[key], ax=ax[key])\n",
    "\n",
    "def draw_error(i):\n",
    "    \"\"\"Draw error at each analysis time step\n",
    "    \"\"\"\n",
    "    field['truth'] = np.loadtxt(os.path.join('inputs_online', f'true_step{i}.txt'))\n",
    "    for j in range(1, dim_ens + 1):\n",
    "        field['fcst'][j-1] = np.loadtxt(os.path.join('outputs', f'ens_{j}_step{i}_for.txt'))\n",
    "        field['ana'][j-1]  = np.loadtxt(os.path.join('outputs', f'ens_{j}_step{i}_ana.txt'))\n",
    "\n",
    "    for j, key in enumerate(spread):\n",
    "        spread[key][i//2 - 1] = field[key].std(0).mean()\n",
    "        RMSE[key][i//2 - 1] = np.sqrt(np.mean((field[key].mean(0) - field['truth'])**2))\n",
    "        lines[2*j].set_ydata(spread[key])\n",
    "        lines[2*j + 1].set_ydata(RMSE[key])\n",
    "\n",
    "    for key in ax:\n",
    "        ax[key].set_title(f'{key} error ({np.round(RMSE[key][i//2 - 1], 3)})')\n",
    "        pc[key].set_array(field[key].mean(0) - field['truth'])\n",
    "\n",
    "    return pc['fcst'], pc['ana'], *lines\n",
    "\n",
    "# make an animation\n",
    "anim = animation.FuncAnimation(fig, draw_error, frames=time, interval=1000, blit=True)\n",
    "plt.close(fig)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM9eUhGPyPp6K8w6XJK3p/O",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
