<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Parallelisation Strategy &#8212; pyPDAF  documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=d1102ebc" />
    <link rel="stylesheet" type="text/css" href="_static/basic.css?v=df3fedd5" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=27fed22d" />
    <script src="_static/documentation_options.js?v=5929fcd5"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Developer Guide" href="develop.html" />
    <link rel="prev" title="Installation" href="install.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="parallelisation-strategy">
<h1>Parallelisation Strategy<a class="headerlink" href="#parallelisation-strategy" title="Link to this heading">¶</a></h1>
<p>PDAF can be run both in serial and parallel.
In either cases, Message Parsing Interface (MPI) is used.
In (py)PDAF, users need to specify the MPI communicators for the model (<code class="docutils literal notranslate"><span class="pre">comm_model</span></code>),
the filter(<code class="docutils literal notranslate"><span class="pre">comm_filter</span></code>), and the coupling (<code class="docutils literal notranslate"><span class="pre">comm_couple</span></code>)
between the model and the filter. These communicators are specified when PDAF is initialised
by <a class="reference internal" href="_autosummary/pyPDAF.PDAF.init.html#pyPDAF.PDAF.init" title="pyPDAF.PDAF.init"><span class="xref myst py py-func"><code class="docutils literal notranslate"><span class="pre">pyPDAF.PDAF.init</span></code></span></a>.
In MPI, a communicator is formed by a group of processes.
The default communicator in MPI is called <code class="docutils literal notranslate"><span class="pre">MPI_COMM_WORLD</span></code>.</p>
<p>If we design a program run by <code class="docutils literal notranslate"><span class="pre">npes_world</span> <span class="pre">=</span> <span class="pre">12</span></code> processes,
the default MPI communicator, <code class="docutils literal notranslate"><span class="pre">MPI_COMM_WORLD</span></code>, controls all these processes.
In (py)PDAF, one can make use of all processes,
or only a handful of these processes.
In the former case, the communicator for an ensemble system
is <code class="docutils literal notranslate"><span class="pre">comm_ens</span> <span class="pre">=</span> <span class="pre">MPI_COMM_WORLD</span></code> with <code class="docutils literal notranslate"><span class="pre">npes_ens</span> <span class="pre">=</span> <span class="pre">12</span></code> processes.
In the latter case, one can choose a few processors (<code class="docutils literal notranslate"><span class="pre">npes_ens</span></code>)
as communicator <code class="docutils literal notranslate"><span class="pre">comm_ens</span></code> dedicated to PDAF, and the rest processors
can be used for other tasks, e.g. I/O operations.
The MPI communicator used by (py)PDAF can be set
by <a class="reference internal" href="_autosummary/pyPDAF.PDAF.set_comm_pdaf.html#pyPDAF.PDAF.set_comm_pdaf" title="pyPDAF.PDAF.set_comm_pdaf"><span class="xref myst py py-func"><code class="docutils literal notranslate"><span class="pre">pyPDAF.PDAF.set_comm_pdaf</span></code></span></a>.</p>
<section id="online-mode">
<h2>Online mode<a class="headerlink" href="#online-mode" title="Link to this heading">¶</a></h2>
<p><img alt="Illustration of online PDAF MPI communicators" src="https://pdaf.awi.de/pics/communicators_PDAFonline.png" /></p>
<p><em>Here is an illustration of the parallel strategy for an online DA system.
The example in this figure uses <code class="docutils literal notranslate"><span class="pre">npes_ens</span> <span class="pre">=</span> <span class="pre">12</span></code> with <code class="docutils literal notranslate"><span class="pre">npes_model</span> <span class="pre">=</span> <span class="pre">npes_filter</span> <span class="pre">=</span> <span class="pre">4</span></code>
where the filtering (filter commnicator) is done on one of the model communicators.
The model is decomposed into 4 sub-domains in this figure. Each coupling communicator
collects state vector from each model process running the same model domain.</em></p>
<section id="model-communicator">
<h3>Model communicator<a class="headerlink" href="#model-communicator" title="Link to this heading">¶</a></h3>
<p>Here, <code class="docutils literal notranslate"><span class="pre">comm_ens</span></code> can be divided into 3 model communicators (<code class="docutils literal notranslate"><span class="pre">model_comm</span></code>), each of which has <code class="docutils literal notranslate"><span class="pre">npes_model</span> <span class="pre">=</span> <span class="pre">4</span></code> processes. In the context of PDAF, each model communicator can perform an independent model run. That is, we have 3 parallel model tasks (<code class="docutils literal notranslate"><span class="pre">n_modeltasks</span> <span class="pre">=</span> <span class="pre">3</span></code>). In this specific example,</p>
<ul class="simple">
<li><p>if we have 3 ensemble members (<code class="docutils literal notranslate"><span class="pre">dim_ens</span> <span class="pre">=</span> <span class="pre">3</span></code>), each model task runs one ensemble member. This is the case in the figure above. This means that the ensemble members local to the model task, <code class="docutils literal notranslate"><span class="pre">dim_ens_l</span> <span class="pre">=</span> <span class="pre">1</span></code>. This setup is called <code class="docutils literal notranslate"><span class="pre">fully</span> <span class="pre">flexible</span></code> setup in PDAF. One should uses functions for <a class="reference internal" href="API.html#fully-parallel-da-algorithms"><span class="std std-ref"><code class="docutils literal notranslate"><span class="pre">fully</span> <span class="pre">parallel</span></code> functions</span></a></p></li>
<li><p>in the case that <code class="docutils literal notranslate"><span class="pre">dim_ens</span> <span class="pre">&gt;</span> <span class="pre">3</span></code>, each model task runs more than one ensemble member serially.  Each model task could also have different number of local ensemble members as <code class="docutils literal notranslate"><span class="pre">dim_ens</span></code> is not necessarily a multiple of <code class="docutils literal notranslate"><span class="pre">n_modeltasks</span></code>. For example, if <code class="docutils literal notranslate"><span class="pre">dim_ens</span> <span class="pre">=</span> <span class="pre">4</span></code>, one model task runs <code class="docutils literal notranslate"><span class="pre">dim_ens_l</span> <span class="pre">=</span> <span class="pre">2</span></code> ensemble members serially and others just runs <code class="docutils literal notranslate"><span class="pre">dim_ens_l</span> <span class="pre">=</span> <span class="pre">1</span></code> ensemble member. In this case, functions for <a class="reference internal" href="API.html#flexible-da-algorithms"><span class="std std-ref"><code class="docutils literal notranslate"><span class="pre">flexible</span></code> functions</span></a> must be used followed by <a class="reference internal" href="_autosummary/pyPDAF.PDAF.get_state.html#pyPDAF.PDAF.get_state" title="pyPDAF.PDAF.get_state"><span class="xref myst py py-func"><code class="docutils literal notranslate"><span class="pre">pyPDAF.PDAF.get_state</span></code></span></a>.</p></li>
</ul>
</section>
<section id="filter-communicator">
<h3>Filter communicator<a class="headerlink" href="#filter-communicator" title="Link to this heading">¶</a></h3>
<p>In our example, one model task runs on <code class="docutils literal notranslate"><span class="pre">npes_model</span> <span class="pre">=</span> <span class="pre">4</span></code> processes. For the sake of efficiency, most physical climate models perform domain decomposition. Under the domain decomposition, the model domain is divided into smaller domains, and each process only simulates a sub-domain. To make the example more concrete, if the model has 2 variables and 44 grid points, each sub-domain will simulate 11 grid points. The model domain decomposition fits well with the concept of domain localisation where the filtering algorithm perform assimilation for each element of the state vector individually, e.g.:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">element</span> <span class="ow">in</span> <span class="n">state_vector</span><span class="p">:</span>
    <span class="n">do_assimilation</span><span class="p">(</span><span class="n">element</span><span class="p">)</span>
</pre></div>
</div>
<p>The domain localisation is great for parallelisation as the assimilation of each element of the state vector is completely independent of each other. In (py)PDAF, one can follow the domain decomposition of the model. In this case, one of the model communicators can be used as a filter communicator (<code class="docutils literal notranslate"><span class="pre">comm_filter</span></code>) where the number of processes performing filtering is <code class="docutils literal notranslate"><span class="pre">npes_filter</span> <span class="pre">=</span> <span class="pre">npes_filter</span> <span class="pre">=</span> <span class="pre">4</span></code>. In the example above, each local domain contains <code class="docutils literal notranslate"><span class="pre">dim_p</span> <span class="pre">=</span> </code><span class="math notranslate nohighlight">\(2 \times 11\)</span> number of elements in the process-local state vector, <code class="docutils literal notranslate"><span class="pre">state_p</span></code>, where the subscript <code class="docutils literal notranslate"><span class="pre">p</span></code> represents arrays on a specific process, or PE-local. Certainly, this means other model communicators are not in use during the filtering stage, but it is a good compromise compared to the increased complexity of redistributing the state vector.</p>
</section>
<section id="coupling-communicator">
<h3>Coupling communicator<a class="headerlink" href="#coupling-communicator" title="Link to this heading">¶</a></h3>
<p>In the filtering stage, ensemble systems must gather model state from each ensemble member to the filtering processes. This is done by a coupling communicator (<code class="docutils literal notranslate"><span class="pre">comm_couple</span></code>), where model processors simulating the same sub-domain are grouped together. The number of processes used for coupling is usually <code class="docutils literal notranslate"><span class="pre">n_modeltasks</span></code>. After the filtering, the coupling communicator is used to distribute the analysis back to the model for following simulations.</p>
</section>
</section>
<section id="offline-mode">
<h2>Offline mode<a class="headerlink" href="#offline-mode" title="Link to this heading">¶</a></h2>
<p>In PDAF, the parallelisation of the offline mode is a simplification of the online mode. In offline mode, the DA program only performs the filtering algorithm. If the filtering is performed in serial, the model, filter and coupling communicator are all given as <code class="docutils literal notranslate"><span class="pre">comm_ens</span></code> with <code class="docutils literal notranslate"><span class="pre">npes_ens</span> <span class="pre">=</span> <span class="pre">1</span></code>.</p>
<p><img alt="Illustration of offline PDAF MPI communicators" src="https://pdaf.awi.de/pics/communicators_PDAFoffline.png" /></p>
<p><em>Here is an illustration of the parallel strategy for an offline DA system. The example in this figure uses <code class="docutils literal notranslate"><span class="pre">npes_ens</span> <span class="pre">=</span> <span class="pre">4</span></code> with <code class="docutils literal notranslate"><span class="pre">npes_model</span> <span class="pre">=</span> <span class="pre">npes_filter</span> <span class="pre">=</span> <span class="pre">4</span></code>. The model is assumed to be decomposed into 4 sub-domains in this figure.</em></p>
<p>For the filtering algorithm supporting parallelisation, if <code class="docutils literal notranslate"><span class="pre">npes_ens</span> <span class="pre">=</span> <span class="pre">4</span></code>, the state vector is partitioned into <code class="docutils literal notranslate"><span class="pre">npes_model</span> <span class="pre">=</span> <span class="pre">npes_filter</span> <span class="pre">=</span> <span class="pre">4</span></code> state vectors that is local to one process, <code class="docutils literal notranslate"><span class="pre">state_p</span></code>, where the subscript <code class="docutils literal notranslate"><span class="pre">p</span></code> represents process-local, or PE-local. This means that each process performs filtering for only a section of the full state vector, and PDAF assumes that the ensemble model is run by <code class="docutils literal notranslate"><span class="pre">npes_model</span> <span class="pre">=</span> <span class="pre">4</span></code> number of processes. The reason for this treatment is explained in <a class="reference internal" href="#filter-communicator">Filter communicator setup in online mode</a>. In this case, <code class="docutils literal notranslate"><span class="pre">comm_filter</span> <span class="pre">=</span> <span class="pre">comm_model</span> <span class="pre">=</span> <span class="pre">comm_ens</span></code>. The coupling communicator is used to collect ensemble from different model tasks to the filter processes in online mode. In the offline mode, the model state is read from disk files, and only one model task is run. Hence, one <code class="docutils literal notranslate"><span class="pre">comm_couple</span></code> corresponds to just one model process.</p>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">pyPDAF</a></h1>



<p class="blurb">A Python interface to Parallel Data Assimilation Framework.</p>




<p>
<iframe src="https://ghbtns.com/github-btn.html?user=yumengch&repo=pyPDAF&type=watch&count=true&size=large&v=2"
  allowtransparency="true" frameborder="0" scrolling="0" width="200px" height="35px"></iframe>
</p>






<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Parallelisation Strategy</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#online-mode">Online mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="#offline-mode">Offline mode</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="develop.html">Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="API.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="API_legacy.html">API (legacy)</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="install.html" title="previous chapter">Installation</a></li>
      <li>Next: <a href="develop.html" title="next chapter">Developer Guide</a></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2022 University of Reading and National Centre for Earth Observation.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.1.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="_sources/parallel.md.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>