{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/yumengch/pyPDAF/blob/main/example/tutorial1_serial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PACl2a3eN4Sy"
   },
   "source": [
    "# Building an Assimilation System with pyPDAF without parallelisation\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBE84vfB5OY6"
   },
   "source": [
    "To get familiar with the design pattern of the e will build a data assimilation system using pyPDAF. pyPDAF is a Python interface to PDAF (Parallel Data Assimilation Framework). The framework is mainly designed for ensemble data assimilation systems with high-dimensional complex weather and climae models. It was used for both research and operational purposes. The Python interface allows for the use of PDAF with Python models.\n",
    "\n",
    "As per its name, the framework is designed with the aim to implement efficient parallelised data assimilation system. In this practical, to illustrate the workflow of PDAF, we provide a step-by-step tutorial on designing a DA system without using parallelisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-maLnxQirCAv"
   },
   "source": [
    "## Install pyPDAF\n",
    "\n",
    "Before discussing the DA system, let us install pyPDAF first. If you are familiar with Python, you might have the package manager `conda` installed. This can be obtained using `anaconda` or `miniconda`. This functionality can be used on Google Colab as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6Es_i7wGBNI"
   },
   "source": [
    "### On local computer:\n",
    "\n",
    "In your terminal or anaconda prompt, run `conda create -n pypdaf -c yumengch -c conda-forge pypdaf jupyter`.\n",
    "\n",
    "You can then open this notebook using the command `jupyter notebook`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSsjNPl-F1On"
   },
   "source": [
    "### On Google Colab (skip this section when you're not using Google Colab):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpxjlTuLSZ8H"
   },
   "source": [
    "The following step will install `conda` on the Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcRt2oa8N3fo"
   },
   "outputs": [],
   "source": [
    "!pip install -q condacolab\n",
    "import condacolab\n",
    "condacolab.install()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXxhlQy_69bd"
   },
   "source": [
    "Now, we can install pyPDAF using conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-o9dcjXOaAJ"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# installation of pyPDAF\n",
    "!conda install -c yumengch -c conda-forge pypdaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNy2ubUw7Bl3"
   },
   "source": [
    "To provide a better view of PDAF output, we have to use wurlitzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cGn7y1usALz"
   },
   "source": [
    "## A short introduction to the example 2D model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHTqZ_SkHyYL"
   },
   "source": [
    "We need to first get the example data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_TOr4rAIAsp",
    "outputId": "91c83738-a657-42aa-c5be-6be69ad9c4fb"
   },
   "outputs": [],
   "source": [
    "\"\"\" this is a way to get all required data without using git and introducing additional libraries\"\"\"\n",
    "import os\n",
    "import urllib.request\n",
    "# create input data directory\n",
    "os.makedirs('inputs_online', exist_ok=True)\n",
    "link_to_files = 'https://raw.githubusercontent.com/PDAF/PDAF/PDAF_V2.1/tutorial/inputs_online'\n",
    "\n",
    "# get the initial truth\n",
    "urllib.request.urlretrieve(f'{link_to_files}/true_initial.txt', os.path.join('inputs_online','true_initial.txt'))\n",
    "\n",
    "for i in range(18):\n",
    "    # get all truth\n",
    "    urllib.request.urlretrieve(f'{link_to_files}/true_step{i+1}.txt', os.path.join('inputs_online', f'true_step{i+1}.txt'))\n",
    "    # get obs\n",
    "    urllib.request.urlretrieve(f'{link_to_files}/obs_step{i+1}.txt', os.path.join('inputs_online',f'obs_step{i+1}.txt'))\n",
    "\n",
    "# get initial ensemble\n",
    "for i in range(9):\n",
    "    urllib.request.urlretrieve(f'{link_to_files}/ens_{i+1}.txt', os.path.join('inputs_online', f'ens_{i+1}.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7ZiAh-xH8Ar"
   },
   "source": [
    "- Spatial Domain: Two dimensional domain grid domain with $(nx \\times ny) = (36 \\times 18)$ grid points\n",
    "- Total steps: we will run this model by `nsteps = 18` time steps\n",
    "\n",
    "With this information, we can define the model variable `field`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHBcpFqo4d-i"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# define the array for model field\n",
    "nsteps = 18 # total time steps\n",
    "nx = 36 # 36 columns\n",
    "ny = 18 # 18 rows\n",
    "# initial condition + 18 time steps, 18 rows and 36 columns\n",
    "field = np.zeros((nsteps + 1, ny, nx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KCHJ4YD-mEC"
   },
   "source": [
    "The initial condition of the model is a sine wave in the diagonal direction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XCiMLV48_U3o"
   },
   "outputs": [],
   "source": [
    "\"\"\"read the initial condition of the model field\"\"\"\n",
    "field[0] = np.loadtxt(os.path.join('inputs_online','true_initial.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cK8GhJ0R4goo"
   },
   "source": [
    "The model shifts the sine wave along the y-axis by one grid point per time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ygfYHHWf_UgI"
   },
   "outputs": [],
   "source": [
    "def step(field):\n",
    "    \"\"\"Roll array elements of i-th time step along a the first axis.\"\"\"\n",
    "    return np.roll(field, shift=1, axis=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5OhqIzCSwNij"
   },
   "source": [
    "### How does the model look like?\n",
    "\n",
    "We can visualise the model evolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cz44hSkm19hg"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "fig = plt.figure('animation')\n",
    "ax = fig.add_subplot(111)\n",
    "pc = ax.pcolormesh(field[0], cmap='coolwarm', vmin=-1, vmax=1)\n",
    "fig.colorbar(pc, ax=ax)\n",
    "\n",
    "def draw_model(i):\n",
    "  \"\"\"Draw each model step\n",
    "  \"\"\"\n",
    "  # run the model\n",
    "  field[i+1] = step(field[i])\n",
    "  pc.set_array(field[i+1])\n",
    "  ax.set_title(f'Model step {i+1}')\n",
    "  return pc,\n",
    "\n",
    "# make an animation\n",
    "anim = animation.FuncAnimation(fig, draw_model, frames=nsteps, interval=1000, blit=True)\n",
    "plt.close(fig)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-mYku6H84ud"
   },
   "source": [
    "## Observations\n",
    "\n",
    "- Select a set of observations at 28 grid points\n",
    "- Observations are sampled from a Gaussian distribution with a standard deviation of 0.5\n",
    "- full 2D field, -999 marks ‘no data’\n",
    "- One observation file at each time step\n",
    "- They're stored in 'obs_step*.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bg1G9Jxo9KpD"
   },
   "outputs": [],
   "source": [
    "# define observation array\n",
    "obs = np.ma.zeros((nsteps, ny, nx))\n",
    "fig = plt.figure('animationObs')\n",
    "ax = fig.add_subplot(111)\n",
    "pc = ax.pcolormesh(obs[0], cmap='coolwarm', vmin=-1, vmax=1)\n",
    "fig.colorbar(pc, ax=ax)\n",
    "def draw_model(i):\n",
    "  \"\"\"Draw obs. at each model step\n",
    "  \"\"\"\n",
    "  obs[i] = np.loadtxt(os.path.join('inputs_online', f'obs_step{i+1}.txt'))\n",
    "  obs[i] = np.ma.masked_where(np.isclose(obs[i], -999.), obs[i])\n",
    "  pc.set_array(obs[i])\n",
    "  ax.set_title(f'Observation at step {i+1}')\n",
    "  return pc,\n",
    "\n",
    "# make an animation\n",
    "anim = animation.FuncAnimation(fig, draw_model, frames=18, interval=1000, blit=True)\n",
    "plt.close(fig)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8yt_y_LNnVt"
   },
   "source": [
    "### Let's check the observation error\n",
    "\n",
    "Is it close to the standard deviation of 0.5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5U5Go0lKMRbH"
   },
   "outputs": [],
   "source": [
    "# calculate the root mean squared obs. err in time\n",
    "err = np.sqrt(np.sum((field[1:] - obs)**2, axis=0)/nsteps)\n",
    "# plot the observation error\n",
    "fig = plt.figure('R')\n",
    "ax = fig.add_subplot(111)\n",
    "pc = ax.pcolormesh(err, cmap='Blues', vmin=0., vmax=1.)\n",
    "ax.set_title(f'Spatial averaged obs. err is {np.round(np.mean(err), 3)}')\n",
    "fig.colorbar(pc, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfRF0cTmPWI4"
   },
   "source": [
    "## Set up a data assimilation system using pyPDAF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HiwEznjFAxEN"
   },
   "source": [
    "We first import necessary packages. In this simple example, we only need pyPDAF, numpy and mpi4py. We will not use parallisation in this example, but we still need mpi4py for running PDAF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\"\"\"For the sake of compatibility of running it in Google Colab, we install wurlitzer here. \n",
    "   This is not essential if you run it on your local computer, but you will need to remove\n",
    "   all code related to wurlitzer in this notebook.\"\"\"\n",
    "# wurlitzer is a package that allows us to see PDAF output\n",
    "!pip install wurlitzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pzlLMXevPF8I"
   },
   "outputs": [],
   "source": [
    "import pyPDAF.PDAF as PDAF\n",
    "import mpi4py.MPI as MPI\n",
    "import numpy as np\n",
    "from wurlitzer import pipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gwe4J1xVfGTK"
   },
   "source": [
    "### Initialise PDAF\n",
    "\n",
    "The initialisation of PDAF tells PDAF the our choice of data assimilation algorithms, ensemble size, inflation factor, and the dimension of the state vector. In this example, for the sake of simplicity, we do not use any localisation and no inflation is applied.\n",
    "\n",
    "The **standard error space transform Kalman filter (ESTKF)** is used with **9** ensemble members. We will estimate the state of every model grid point, which gives us a state vector with the size of nx × ny = 36 × 18 = 648"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g4wDabInPY7O"
   },
   "outputs": [],
   "source": [
    "# using error space transform Kalman filter (ESTKF)\n",
    "filtertype = 6\n",
    "# standard form\n",
    "subtype = 0\n",
    "# dimension of the state vector\n",
    "# if model is parallelised, this is the dimension of state vector on each process\n",
    "dim_state_p = nx*ny\n",
    "# number of ensemble members\n",
    "dim_ens = 9\n",
    "# forget factor\n",
    "forget_factor = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrIrGKSvWrcu"
   },
   "source": [
    "In addition to the basic information, PDAF also asks for an **initial ensemble**. This information is given by the user-supplied function. These functions have fixed interface. Therefore\n",
    "- the input arguments and return variables should not be changed.\n",
    "\n",
    "The initial ensemble is read from given text files here. In real applications, we may need to use algorithms to generate perturbations to create an initial ensemble. For the sake of conciseness, documentation of the input arguments and return variable of this function can be found in [pyPDAF documentation](https://yumengch.github.io/pyPDAF/UserFunc.html#pyPDAF.UserFunc.py__init_ens_pdaf) and [PDAF documentation](https://pdaf.awi.de/trac/wiki/U_init_ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LIksYDywTetx"
   },
   "outputs": [],
   "source": [
    "def init_ens_pdaf(filtertype, dim_p, dim_ens, state_p, uinv, ens_p, status_pdaf):\n",
    "    \"\"\"Here, only ens_p variable matters while dim_p and dim_ens defines the\n",
    "    size of the variables. uinv, state_p are not used in this example.\n",
    "\n",
    "    status_pdaf is used to handle errors which we will not do it in this example.\n",
    "    \"\"\"\n",
    "    for i in range(dim_ens):\n",
    "        ens_p[:, i] = np.loadtxt(os.path.join('inputs_online', f'ens_{i+1}.txt')).ravel()\n",
    "    return state_p, uinv, ens_p, status_pdaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tf8x0NGAfEz2"
   },
   "source": [
    "With these information, we can call PDAF function [`PDAF.init`](https://yumengch.github.io/pyPDAF/PDAF.html#pyPDAF.PDAF.init) to initialise the DA system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tb76E2-OPOPU"
   },
   "outputs": [],
   "source": [
    "# this gives the verbose level of the PDAF, here we use 3 which is very verbose\n",
    "screen = 3\n",
    "# current step of the model which is 0\n",
    "current_step = 0\n",
    "with pipes() as (out, err):\n",
    "    _, _, status = PDAF.init(filtertype, subtype, current_step,\n",
    "                            np.array([dim_state_p, dim_ens]),\n",
    "                            np.array([forget_factor, ]),\n",
    "                            MPI.COMM_WORLD.py2f(), MPI.COMM_WORLD.py2f(),\n",
    "                            MPI.COMM_WORLD.py2f(),\n",
    "                            task_id=1, n_modeltasks=1, in_filterpe=True,\n",
    "                            py__init_ens_pdaf=init_ens_pdaf,\n",
    "                            in_screen=screen)\n",
    "# print PDAF screen output\n",
    "print (out.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXpuKapDbnZN"
   },
   "source": [
    "### Distribution of the ensemble from PDAF\n",
    "After PDAF initialisation, PDAF should distribute the ensemble back to the model for the following forecast. Three user-supplied functions are needed:\n",
    "1. PDAF should distribute the initial ensemble to the model for following model forecasts [(`distribute_state`)](https://yumengch.github.io/pyPDAF/UserFunc.html#pyPDAF.UserFunc.py__distribute_state_pdaf)\n",
    "2. The ensemble might need some processing before being distributed to the model [(`initial_process`)](https://yumengch.github.io/pyPDAF/UserFunc.html#pyPDAF.UserFunc.py__prepoststep_pdaf)\n",
    "3. PDAF will need to know how many steps the forecast will take. In other words, when do we do the next data assimilation analysis. That is, when will the next observation arrive? [(`next_observation`)](https://yumengch.github.io/pyPDAF/UserFunc.html#pyPDAF.UserFunc.py__next_observation_pdaf)\n",
    "\n",
    "In this example, we define a class of `PDAF_distributor` for these user-supplied functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5L5eR_B6ZVq"
   },
   "outputs": [],
   "source": [
    "class PDAF_distributor:\n",
    "    def __init__(self, nx, ny, dim_ens):\n",
    "        # counter for the i-th ensemble member when distribute\n",
    "        self.i_ens_pdaf = 0\n",
    "        # define the model field based on the ensemble\n",
    "        self.nx, self.ny = nx, ny\n",
    "        self.field = np.zeros((dim_ens, ny, nx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_44HquA6aXw"
   },
   "source": [
    "\n",
    "In this non-parallel code, PDAF will distribute ensemble members one by one with `distribute_state` method. It is useful to keep a counter for the index of currently distributed ensemble member."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cQCHlYMxc-Rp"
   },
   "outputs": [],
   "source": [
    "class PDAF_distributor(PDAF_distributor):\n",
    "    def distribute_state(self, dim_p, state_p):\n",
    "        \"\"\"PDAF will distribute state vector (state_p) to model field\n",
    "        \"\"\"\n",
    "        self.field[self.i_ens_pdaf] = state_p[:].reshape((self.ny, self.nx))\n",
    "        self.i_ens_pdaf += 1\n",
    "        return state_p\n",
    "\n",
    "    def reset_ens_index(self):\n",
    "        \"\"\"reset ensemble index to 0\n",
    "        \"\"\"\n",
    "        self.i_ens_pdaf = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzC5mODK6fxJ"
   },
   "source": [
    "In the initial-processing, we only show screen output of root mean squared error based on sampled variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1lgaGqK6k7h"
   },
   "outputs": [],
   "source": [
    "class PDAF_distributor(PDAF_distributor):\n",
    "    def initial_process(self, step, dim_p, dim_ens, dim_ens_p, dim_obs_p, state_p, uinv, ens_p, flag):\n",
    "        \"\"\"initial processing of the ensemble before it is distributed to model fields\n",
    "        \"\"\"\n",
    "        print (f'RMS error according to sampled variance: {np.sqrt(np.mean(np.var(ens_p, axis=1)))}')\n",
    "        return state_p, uinv, ens_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4h3TACpstgl"
   },
   "source": [
    "When obtaining the initial ensemble, users also need to provide information about when do we do the next analysis based on the arrival of the new observations. In our case, we have observations for each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DbmL_zvjYJaa"
   },
   "outputs": [],
   "source": [
    "class PDAF_distributor(PDAF_distributor):\n",
    "    def next_observation(self, stepnow, nsteps, doexit, time):\n",
    "        # next observation will arrive at `nsteps' steps\n",
    "        nsteps = 2\n",
    "        # doexit = 0 means that PDAF will continue to distribute state\n",
    "        # to model for further integrations\n",
    "        doexit = 0\n",
    "        # model time is not used here as we only use steps to define the time\n",
    "        return nsteps, doexit, time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bu157WPn7CiJ"
   },
   "source": [
    "Here, we call pyPDAF's [`get_state`](https://yumengch.github.io/pyPDAF/PDAF.html#pyPDAF.PDAF.get_state) function where it will also execute our user-supplied functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HX5TjUCNYTVh"
   },
   "outputs": [],
   "source": [
    "status = 0\n",
    "distributor = PDAF_distributor(nx, ny, dim_ens)\n",
    "distributor.reset_ens_index()\n",
    "steps_for = 0\n",
    "doexit = 0\n",
    "# loop over all dimensions\n",
    "for i in range(dim_ens):\n",
    "    with pipes() as (out, err):\n",
    "        steps_for, time, doexit, status = PDAF.get_state(steps_for, doexit,\n",
    "                                          distributor.next_observation,\n",
    "                                          distributor.distribute_state,\n",
    "                                          distributor.initial_process,\n",
    "                                          status)\n",
    "    print (out.read())\n",
    "# put model variable in distributor back to model\n",
    "field = distributor.field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLY8FDiL4NdC"
   },
   "source": [
    "The `get_state` function returns `steps_for` variable, which is the number of forecast time steps before the next analysis. This is equal to the `nsteps` defined in `next_observation` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGeTvOG2u8LU"
   },
   "source": [
    "### Forward data assimilation system\n",
    "\n",
    "Data assimilation combines the model forecast and the observations. Hence, the data assimilation system does two things\n",
    " - model forecast\n",
    "  - in this example, the `step` function is used as we defined [previously](#scrollTo=9cGn7y1usALz&line=2&uniqifier=1)\n",
    " - data assimilation, where PDAF carries out the following operations based on user-supplied functions\n",
    "  - collecting state vector from model variables\n",
    "  - collecting observations\n",
    "  - distributing analysis back to model variables\n",
    "\n",
    "At each analysis step, PDAF must collect the new forecast from the model and handle observations. To ensure the flexibility of the framework, these information depends on the user-supplied functions. As observations and models are always case-specific.\n",
    "\n",
    "Here, we define two classes, `PDAF_collector` and `Obs`. The `PDAF_collector` obtains the model forecast and the `Obs` will use the `Observation Module Infrastructure` in PDAF, a scheme to ease the difficulty in handling observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-lmJYuKLE4z"
   },
   "outputs": [],
   "source": [
    "class PDAF_collector:\n",
    "    def __init__(self, nx, ny, field):\n",
    "        # counter for the i-th ensemble member when distribute\n",
    "        self.i_ens_pdaf = 0\n",
    "        self.nx = nx\n",
    "        self.ny = ny\n",
    "        # define the model field based on the ensemble\n",
    "        self.field = field\n",
    "\n",
    "class Obs:\n",
    "    def __init__(self, i_obs):\n",
    "        # i_obs-th observations in the system starting from 1\n",
    "        self.i_obs = i_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PcPa7_IwLuFT"
   },
   "source": [
    "#### Collecting forecast\n",
    "Before going into the details of observation handling, we first get functions that collects the model forecast [(`collect_state_pdaf`)](https://yumengch.github.io/pyPDAF/UserFunc.html#pyPDAF.UserFunc.py__collect_state) and preprocess the ensemble before the data assimilation. In the pre-processing step, we only calculate the forecast error and save the forecast ensemble. After the completion of analysis, the analysis will be postprocessed before being distributed to the model. For example, this could be dealing with bounded variables, or some transformation of the state vector. The pre- and post- processing are handled in [(`preprocess`)](https://yumengch.github.io/pyPDAF/UserFunc.html#pyPDAF.UserFunc.py__prepoststep_pdaf), where argument `step` determines whether it is pre-processing or post-processing. `step` < 0 represents pre-processing while `step` > 0 represents post-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbv-1b7GsxU1"
   },
   "outputs": [],
   "source": [
    "class PDAF_collector(PDAF_collector):\n",
    "    def collect_state(self, dim_p, state_p):\n",
    "        \"\"\"PDAF will collect state vector (state_p) from model field\n",
    "        \"\"\"\n",
    "        state_p[:] = self.field[self.i_ens_pdaf].ravel()\n",
    "        self.i_ens_pdaf += 1\n",
    "        return state_p\n",
    "\n",
    "    def reset_ens_index(self):\n",
    "        \"\"\"reset ensemble index to 0\n",
    "        \"\"\"\n",
    "        self.i_ens_pdaf = 0\n",
    "\n",
    "    def preprocess(self, step, dim_ens, ens_p):\n",
    "        \"\"\"preprocessing of the ensemble before it is used by DA algorithms\n",
    "        \"\"\"\n",
    "        print (f'Forecast RMS error according to sampled variance: {np.sqrt(np.mean(np.var(ens_p, axis=1)))}')\n",
    "        for i in range(dim_ens):\n",
    "            np.savetxt(os.path.join('outputs', f'ens_{i+1}_step{-step}_for.txt') , ens_p[:, i].reshape((self.ny, self.nx)) )\n",
    "\n",
    "    def postprocess(self, step, dim_ens, ens_p):\n",
    "        \"\"\"initial processing of the ensemble before it is distributed to model fields\n",
    "        \"\"\"\n",
    "        print (f'Analysis RMS error according to sampled variance: {np.sqrt(np.mean(np.var(ens_p, axis=1)))}')\n",
    "        for i in range(dim_ens):\n",
    "            np.savetxt(os.path.join('outputs', f'ens_{i+1}_step{step}_ana.txt' ), ens_p[:, i].reshape((self.ny, self.nx)) )\n",
    "\n",
    "    def prepostprocess(self, step, dim_p, dim_ens, dim_ens_p, dim_obs_p, state_p, uinv, ens_p, flag):\n",
    "        if step < 0:\n",
    "          self.preprocess(step, dim_ens, ens_p)\n",
    "        else:\n",
    "          self.postprocess(step, dim_ens, ens_p)\n",
    "        return state_p, uinv, ens_p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1N-wjlkH3fSp"
   },
   "source": [
    "#### Handling observations\n",
    "Another essential ingredient of data assimilation is observation. Here, user-supplied functions give all information about the observations to PDAF. We use the OMI scheme in PDAF to handle observations. Without any localisations, only two user-supplied functions are required with the OMI scheme.\n",
    "\n",
    "Before we use the OMI scheme, we need to provide the number of observation types by [`PDAF.omi_init`](https://yumengch.github.io/pyPDAF/PDAF.html#pyPDAF.PDAF.omi_init). Here, we use only one type of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YiHIwd7tDHW"
   },
   "outputs": [],
   "source": [
    "PDAF.omi_init(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIExW3FhspP9"
   },
   "source": [
    "In this very simple example, it may look a bit verbose, but it can be useful for more complex systems.\n",
    "\n",
    "The OMI has four mandatory properties:\n",
    "- [`doassim`](https://yumengch.github.io/pyPDAF/PDAF.html#pyPDAF.PDAF.omi_set_doassim): whether this observation is assimilated. If `doassim = 1`, this observation will be assimilated. If `doassim = 0`, it will not be assimilated.\n",
    "- [`disttype`](https://yumengch.github.io/pyPDAF/PDAF.html#pyPDAF.PDAF.omi_set_disttype): In localisation, how do we calculate the distance between grid points? e.g., Cartesian, geographic, or great circle distance on a sphere. We do not use localisation in this example, but we still have to provide this option.\n",
    "- [`ncoord`](https://yumengch.github.io/pyPDAF/PDAF.html#pyPDAF.PDAF.omi_set_ncoord): Number of coordinates used for computation in localisation. In our example, as we have a 2D domain, the number should be 2.\n",
    "- [`id_obs_p`](https://yumengch.github.io/pyPDAF/PDAF.html#pyPDAF.PDAF.omi_set_id_obs_p): Indices of observed field in state vector. This is a 2D array that should have the same length as the observector vector for each dimension. If the observations do not need interpolation (e.g., observations are co-located with model grid points), the first dimension is 1. In this case, if the i-th observation is at the j-th element of the state vector, the i-th element of `id_obs_p` is `j`. If interpolation is needed, each dimension is the adjacent model grid points.\n",
    "\n",
    "In the PDAF, these properties can be given to derived type `obs_f`. In the pyPDAF, setter functions are provided. PDAFomi collects the observation vector,  error variance, and the spatial coordinate of the observations in the [`PDAF.omi_gather_obs`](https://yumengch.github.io/pyPDAF/PDAF.html#pyPDAF.PDAF.omi_gather_obs) function. This function returns the dimension of the full observation vector if observation reading are performed in parallel. Therefore, we put this function call in the user-supplied function, [`init_dim_obs`](https://yumengch.github.io/pyPDAF/UserFunc.html#pyPDAF.UserFunc.py__init_dim_obs_pdaf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-7PUdExu6FO"
   },
   "outputs": [],
   "source": [
    "class Obs(Obs):\n",
    "    def init_dim(self, step, dim_obs):\n",
    "        # We always assimilate the observation\n",
    "        PDAF.omi_set_doassim(self.i_obs, 1)\n",
    "        # Type of distance computation to use for localization\n",
    "        # It is mandatory for OMI even if we don't use localisation\n",
    "        PDAF.omi_set_disttype(self.i_obs, 0)\n",
    "        # Number of coordinates use for distance computation\n",
    "        PDAF.omi_set_ncoord(self.i_obs, 2)\n",
    "\n",
    "        # read observations\n",
    "        obs = np.loadtxt(os.path.join('inputs_online', f'obs_step{step}.txt'))\n",
    "        # get the dimension of the model grid\n",
    "        ny, nx = obs.shape\n",
    "        # flatten the observations\n",
    "        obs = obs.ravel()\n",
    "        # a mask for observed gridpoints\n",
    "        condition = np.logical_not(np.isclose(obs, -999))\n",
    "\n",
    "        # observation vector\n",
    "        y = obs[condition]\n",
    "\n",
    "        # The relationship between observation and state vector\n",
    "        # we only have 28 osbervations and each observation corresponds to\n",
    "        # the grid point of one element in the state vector\n",
    "        # id_obs_p gives the indices of observed field in state vector\n",
    "        # the id starts from 1\n",
    "        id_obs_p = np.zeros((1, len(y)))\n",
    "        id_obs_p[0] = np.arange(1, len(obs) + 1, dtype=int)[condition]\n",
    "        PDAF.omi_set_id_obs_p(self.i_obs, id_obs_p)\n",
    "\n",
    "        # inverse of observation variance\n",
    "        ivar_obs_p = 1./0.5/0.5*np.ones_like(y)\n",
    "\n",
    "        # coordinate of each observations\n",
    "        ocoord_p = np.zeros((2, len(y)))\n",
    "        ocoord_p[0] = np.tile(np.arange(nx), ny)[condition]\n",
    "        ocoord_p[1] = np.repeat(np.arange(ny), nx)[condition]\n",
    "\n",
    "        # not being used here, only used for localisation\n",
    "        local_range = 0.\n",
    "        dim_obs = PDAF.omi_gather_obs(self.i_obs, y,\n",
    "                                     ivar_obs_p, ocoord_p, local_range)\n",
    "        return dim_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXejHv4bl9qp"
   },
   "source": [
    "The other user-supplied function in this example will be the observation operator [(`obs_op`)](https://yumengch.github.io/pyPDAF/UserFunc.html#pyPDAF.UserFunc.py__obs_op_pdaf). In this simple example, the state vector in the observation space can be conviently obtained by the OMI scheme using the information provided in the `init_dim_obs` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DBfL7xBQl-Nt"
   },
   "outputs": [],
   "source": [
    "class Obs(Obs):\n",
    "    def op(self, step, dim_p, dim_obs_p, state_p, ostate):\n",
    "        \"\"\"observation operator\n",
    "        \"\"\"\n",
    "        return PDAF.omi_obs_op_gridpoint(self.i_obs, state_p, ostate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3V8ogKsh8Nq"
   },
   "source": [
    "#### Forward loop\n",
    "\n",
    "Now, we can write code for the forward DA system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51H_l8TuwSBk"
   },
   "outputs": [],
   "source": [
    "os.makedirs('outputs', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PIz-l6zaZMNo"
   },
   "outputs": [],
   "source": [
    "# create directory to\n",
    "current_step = 0\n",
    "# full DA system integration loop\n",
    "while current_step < nsteps:\n",
    "\n",
    "    # model integration\n",
    "    for _ in range(steps_for):\n",
    "        field = step(field)\n",
    "        current_step += 1\n",
    "\n",
    "    # PDAF does assimilation\n",
    "    collector = PDAF_collector(nx, ny, field)\n",
    "    obs = Obs(1)\n",
    "    collector.reset_ens_index()\n",
    "    for i in range(dim_ens):\n",
    "        with pipes() as (out, err):\n",
    "            status = PDAF.omi_put_state_global(collector.collect_state,\n",
    "                                      obs.init_dim, obs.op,\n",
    "                                      collector.prepostprocess)\n",
    "        print (out.read())\n",
    "\n",
    "    # PDAF distribute analysis back to model\n",
    "    distributor = PDAF_distributor(nx, ny, dim_ens)\n",
    "    distributor.reset_ens_index()\n",
    "    for i in range(dim_ens):\n",
    "        with pipes() as (out, err):\n",
    "            # here, the distributor does not call init_process function at all\n",
    "            steps_for, time, doexit, status = PDAF.get_state(steps_for, doexit,\n",
    "                                              distributor.next_observation,\n",
    "                                              distributor.distribute_state,\n",
    "                                              distributor.initial_process,\n",
    "                                              status)\n",
    "        print (out.read())\n",
    "    field = distributor.field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTBiN9NAR1M1"
   },
   "source": [
    "### Does analysis look better than forecast?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0qoAUD5-R0BH"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "dim_ens=9\n",
    "ny, nx = 18, 36\n",
    "# define diagnotics and model fields\n",
    "spread = {'fcst': np.zeros(9), 'ana': np.zeros(9)}\n",
    "RMSE = {'fcst': np.zeros(9), 'ana': np.zeros(9)}\n",
    "field = {'truth': np.zeros((ny, nx)),\n",
    "         'fcst': np.zeros((dim_ens, ny, nx)),\n",
    "         'ana': np.zeros((dim_ens, ny, nx))\n",
    "         }\n",
    "for key in spread:\n",
    "    spread[key][:] = np.nan\n",
    "    RMSE[key][:] = np.nan\n",
    "# time\n",
    "time = np.arange(2, 20, 2)\n",
    "\n",
    "# get figure\n",
    "fig = plt.figure('err')\n",
    "w, h = fig.get_size_inches()\n",
    "fig.set_size_inches(2*w, 2*h)\n",
    "# define the time series plot\n",
    "ax = fig.add_subplot(212)\n",
    "ax.set_title('Time series of the ensemble spread and RMSE')\n",
    "ax.set_ylim([0., 1.2])\n",
    "ax.set_xlim([time[0] - 1, time[-1] + 1])\n",
    "lines = []\n",
    "for key, c in zip(spread, ['k', 'r']):\n",
    "    line, = ax.plot(time, spread[key], color=c, linestyle='dashed',label=f'{key} spread')\n",
    "    lines.append(line)\n",
    "    line, = ax.plot(time, RMSE[key], color=c, linestyle='solid',label=f'{key} RMSE')\n",
    "    lines.append(line)\n",
    "ax.legend()\n",
    "# define pcolormesh plots\n",
    "ax = {'fcst': fig.add_subplot(221), 'ana': fig.add_subplot(222),}\n",
    "pc = dict()\n",
    "for key in ax:\n",
    "    pc[key] = ax[key].pcolormesh(field[key].mean(0) - field['truth'],\n",
    "                                 cmap='coolwarm', vmin=-.06, vmax=.06)\n",
    "    fig.colorbar(pc[key], ax=ax[key])\n",
    "\n",
    "def draw_error(i):\n",
    "    \"\"\"Draw error at each analysis time step\n",
    "    \"\"\"\n",
    "    field['truth'] = np.loadtxt(os.path.join('inputs_online', f'true_step{i}.txt'))\n",
    "    for j in range(1, dim_ens + 1):\n",
    "        field['fcst'][j-1] = np.loadtxt(os.path.join('outputs', f'ens_{j}_step{i}_for.txt'))\n",
    "        field['ana'][j-1]  = np.loadtxt(os.path.join('outputs', f'ens_{j}_step{i}_ana.txt'))\n",
    "\n",
    "    for j, key in enumerate(spread):\n",
    "        spread[key][i//2 - 1] = field[key].std(0).mean()\n",
    "        RMSE[key][i//2 - 1] = np.sqrt(np.mean((field[key].mean(0) - field['truth'])**2))\n",
    "        lines[2*j].set_ydata(spread[key])\n",
    "        lines[2*j + 1].set_ydata(RMSE[key])\n",
    "\n",
    "    for key in ax:\n",
    "        ax[key].set_title(f'{key} error ({np.round(RMSE[key][i//2 - 1], 3)})')\n",
    "        pc[key].set_array(field[key].mean(0) - field['truth'])\n",
    "\n",
    "    return pc['fcst'], pc['ana'], *lines\n",
    "\n",
    "# make an animation\n",
    "anim = animation.FuncAnimation(fig, draw_error, frames=time, interval=1000, blit=True)\n",
    "plt.close(fig)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM9eUhGPyPp6K8w6XJK3p/O",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
